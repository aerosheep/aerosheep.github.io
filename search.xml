<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>爬虫</title>
      <link href="/2022/01/21/%E7%88%AC%E8%99%AB/"/>
      <url>/2022/01/21/%E7%88%AC%E8%99%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="Day02"><a href="#Day02" class="headerlink" title="Day02"></a>Day02</h1><h2 id="爬虫第一课"><a href="#爬虫第一课" class="headerlink" title="爬虫第一课"></a>爬虫第一课</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、爬取搜狗首页的数据</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> response</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 第一步指定url</span></span><br><span class="line">    url = <span class="string">&#x27;https://www.sogou.com/&#x27;</span></span><br><span class="line">    <span class="comment"># 第二步发起请求.get方法会返回一个响应对象</span></span><br><span class="line">    requests.get(url=url)</span><br><span class="line">    <span class="comment"># 第三步获取相应数据.text返回的是字符串形式的响应数据</span></span><br><span class="line">    page_text = requests.text</span><br><span class="line">    <span class="built_in">print</span>(page_text)</span><br><span class="line">    <span class="comment"># 持久化存储</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./sogou.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(<span class="built_in">str</span>(page_text))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;爬取数据结束！！！&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="爬虫第二课"><a href="#爬虫第二课" class="headerlink" title="爬虫第二课"></a>爬虫第二课</h2><p>UA伪装：让爬虫对应的请求载体身份标识伪装成某一款浏览器</p><p>User-Agent:</p><p>headers = {‘User-Agent’:…….}</p><h2 id="爬虫第三课"><a href="#爬虫第三课" class="headerlink" title="爬虫第三课"></a>爬虫第三课</h2><p>chrome浏览器的F12开发者工具的Network里面的XHR对应的是“阿贾克斯”请求数据包</p><p><img src="https://gitee.com/polaris_tyh/pic/raw/master/img/image-20220102212157387.png" alt="image-20220102212157387"></p><p>“阿贾克斯”请求：页面变了但是http地址没变。</p><h3 id="案例："><a href="#案例：" class="headerlink" title="案例："></a>案例：</h3><h4 id="1、爬取百度翻译的数据"><a href="#1、爬取百度翻译的数据" class="headerlink" title="1、爬取百度翻译的数据"></a>1、爬取百度翻译的数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取百度翻译的数据</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 1、指定url</span></span><br><span class="line">    post_url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line">    <span class="comment"># 2、进行UA伪装</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 3、post请求参数处理（同get请求一致）</span></span><br><span class="line">    word = <span class="built_in">input</span>(<span class="string">&#x27;请输入想查询的单词:&#x27;</span>)</span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;kw&#x27;</span>: word</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment"># 4、请求发送</span></span><br><span class="line">    response = requests.post(url=post_url, data=data, headers=headers)</span><br><span class="line">    <span class="comment"># 5、获取响应数据：json()方法返回的是obj(如果确认响应数据是json类型的，才可以使用json())</span></span><br><span class="line">    dic_obj = response.json()</span><br><span class="line">    <span class="comment"># print(dic_obj)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 6、持久化存储</span></span><br><span class="line">    fileName = word + <span class="string">&#x27;.json&#x27;</span></span><br><span class="line">    fp = <span class="built_in">open</span>(fileName, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    json.dump(dic_obj, fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;完成!&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="2、豆瓣电影数据获取"><a href="#2、豆瓣电影数据获取" class="headerlink" title="2、豆瓣电影数据获取"></a>2、豆瓣电影数据获取</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 豆瓣电影爬取</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    url = <span class="string">&quot;https://movie.douban.com/j/chart/top_list&quot;</span></span><br><span class="line">    param = &#123;</span><br><span class="line">        <span class="string">&#x27;type&#x27;</span>: <span class="string">&#x27;24&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;interval_id&#x27;</span>: <span class="string">&#x27;100:90&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;action&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: <span class="string">&#x27;1&#x27;</span>, <span class="comment"># 从库中的第几部电影去取</span></span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>: <span class="string">&#x27;20&#x27;</span>, <span class="comment"># 一次取出多少个</span></span><br><span class="line">    &#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    response = requests.get(url=url, params=param, headers=header)</span><br><span class="line"></span><br><span class="line">    list_data = response.json()</span><br><span class="line"></span><br><span class="line">    fp = <span class="built_in">open</span>(<span class="string">&#x27;./douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    json.dump(list_data, fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h4 id="3、爬虫作业：爬取KFC餐厅response信息"><a href="#3、爬虫作业：爬取KFC餐厅response信息" class="headerlink" title="3、爬虫作业：爬取KFC餐厅response信息"></a>3、爬虫作业：爬取KFC餐厅response信息</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取KFC餐厅信息数据</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    url = <span class="string">&#x27;http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=keyword&#x27;</span></span><br><span class="line">    keyword = <span class="built_in">input</span>(<span class="string">&#x27;请输入想要查询的城市:&#x27;</span>)</span><br><span class="line">    param = &#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;keyword&#x27;</span>: keyword,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;10&#x27;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    response = requests.get(url=url, params=param, headers=headers).text</span><br><span class="line">    fileName = keyword+ <span class="string">&#x27;.text&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(fileName, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(<span class="built_in">str</span>(response))</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;结束&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="4、国家药品监督管理局"><a href="#4、国家药品监督管理局" class="headerlink" title="4、国家药品监督管理局"></a>4、国家药品监督管理局</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 爬取国家药品监督管理局的数据</span></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># UA封装：请求头</span></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># url获取</span></span><br><span class="line">    url = <span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsList&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 参数的封装</span></span><br><span class="line">    i = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入想查到的页数：&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, i+<span class="number">1</span>):</span><br><span class="line">        page = <span class="built_in">int</span>(page)</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;on&#x27;</span>: <span class="string">&#x27;true&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;page&#x27;</span>: page,</span><br><span class="line">            <span class="string">&#x27;pageSize&#x27;</span>: <span class="string">&#x27;15&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;productName&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;conditionType&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;applyname&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">            <span class="string">&#x27;applysn&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment"># 对id数据进行处理</span></span><br><span class="line">        id_list = []  <span class="comment"># 用来存储每个企业所对应的id</span></span><br><span class="line">        json_ids = requests.post(url=url, data=data, headers=headers).json()</span><br><span class="line">        <span class="keyword">for</span> dic <span class="keyword">in</span> json_ids[<span class="string">&#x27;list&#x27;</span>]:</span><br><span class="line">            id_list.append(dic[<span class="string">&#x27;ID&#x27;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 持久化存储</span></span><br><span class="line">    all_Data_list = [] <span class="comment"># 新建一个列表用来存储</span></span><br><span class="line">    post_url = <span class="string">&#x27;http://scxk.nmpa.gov.cn:81/xk/itownet/portalAction.do?method=getXkzsById&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">id</span> <span class="keyword">in</span> id_list:</span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;id&#x27;</span>: <span class="built_in">id</span></span><br><span class="line">        &#125;</span><br><span class="line">        detail_json = requests.post(url=url, headers=headers, data=data).json()</span><br><span class="line">        all_Data_list.append(detail_json)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 持久化存储</span></span><br><span class="line">    fp = <span class="built_in">open</span>(<span class="string">&#x27;./国家药品监督管理局数据.txt&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    json.dump(all_Data_list, fp=fp, ensure_ascii=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;结束&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="Day03"><a href="#Day03" class="headerlink" title="Day03"></a>Day03</h1><h4 id="聚焦爬虫：爬取页面中指定的页面内容。"><a href="#聚焦爬虫：爬取页面中指定的页面内容。" class="headerlink" title="聚焦爬虫：爬取页面中指定的页面内容。"></a>聚焦爬虫：爬取页面中指定的页面内容。</h4><p>​        编码流程：</p><p>​                1、指定url</p><p>​                2、发起请求</p><p>​                3、获取响应数据</p><p>​                4、持久化存储</p><h4 id="数据解析分类："><a href="#数据解析分类：" class="headerlink" title="数据解析分类："></a>数据解析分类：</h4><p>​        re正则表达式</p><p>​        bs4 (beautifulsoup)</p><p>​        xpath    最常用最便捷高效的一种解析方式,通用性最强</p><p>​        xpath &gt; bs4 &gt; re正则表达式</p><h4 id="数据分析原理概述："><a href="#数据分析原理概述：" class="headerlink" title="数据分析原理概述："></a>数据分析原理概述：</h4><p>​        解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储</p><p>​        1、进行指定标签的定位</p><p>​        2、标签或者标签对应的属性中存储的数据值进行提取（解析）</p><h3 id="正则表达式："><a href="#正则表达式：" class="headerlink" title="正则表达式："></a>正则表达式：</h3><p>Regular Expression，正则表达式，一种使用表达式的方式对字符串进行匹配的语法规则。</p><p>我们抓取到的网页源代码本质上就是一个超长的字符串，想从里面提取内容。用正则再适合不过了。</p><p>正则的优点：速度快，效率高，准确性高</p><p>正则的缺点：新手上手难度有点高</p><p><img src="https://gitee.com/polaris_tyh/pic/raw/master/img/image-20211124214938324.png" alt="image-20211124214938324"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>.   匹配除换行符以外的任意字符， 未来在python的re模块中是一个坑</span><br><span class="line"><span class="number">2</span>\w   匹配字母或数字或下划线</span><br><span class="line"><span class="number">3</span>\s   匹配任意的空白符</span><br><span class="line"><span class="number">4</span>\d   匹配数字</span><br><span class="line"><span class="number">5</span>\n   匹配一个换行符</span><br><span class="line"><span class="number">6</span>\t   匹配一个制表符</span><br><span class="line"><span class="number">7</span>^   匹配字符串的开始</span><br><span class="line"><span class="number">8</span>$   匹配字符串的结尾</span><br><span class="line"><span class="number">9</span>\W   匹配 非 字母或数字或下划线</span><br><span class="line"><span class="number">10</span>\D   匹配 非 数字</span><br><span class="line"><span class="number">11</span>\S   匹配 非 空白符</span><br><span class="line"><span class="number">12</span>a|b   匹配字符a或字符b</span><br><span class="line"><span class="number">13</span>()   匹配括号内的表达式，也表示一个组</span><br><span class="line"><span class="number">14</span>[..]   匹配字符组中的字符</span><br><span class="line"><span class="number">15</span>[^..]  匹配除了字符组中字符的所有字符</span><br><span class="line"></span><br><span class="line">量词：</span><br><span class="line"><span class="number">1</span>*   重复零次或更多次</span><br><span class="line"><span class="number">2</span>+   重复一次或更多次</span><br><span class="line"><span class="number">3</span>?   重复零次或一次</span><br><span class="line"><span class="number">4</span>&#123;n&#125;   重复n次</span><br><span class="line"><span class="number">5</span>&#123;n,&#125;   重复n次或更多次</span><br><span class="line"><span class="number">6</span>&#123;n,m&#125;  重复n次到m次</span><br><span class="line"></span><br><span class="line">贪婪匹配和惰性匹配：</span><br><span class="line"><span class="number">1</span>.*   贪婪匹配，尽可能多的去匹配结果（匹配到最远的结尾）</span><br><span class="line"><span class="number">2</span>.*?    惰性匹配，尽可能少的去匹配结果（匹配到最近的结尾结束，然后再进行下一次检测直到结束）</span><br></pre></td></tr></table></figure><h3 id="re模块："><a href="#re模块：" class="headerlink" title="re模块："></a>re模块：</h3><h5 id="re-findall："><a href="#re-findall：" class="headerlink" title="re.findall："></a>re.findall：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">result = re.findall(<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;我是一个abcdefag&#x27;</span>)</span><br><span class="line"><span class="comment"># .findall(匹配全部字串，返回全部内容，类型为数组)</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">result = re.findall(<span class="string">r&#x27;\d+&#x27;</span>, <span class="string">&#x27;我今年18岁，我存款有2000元&#x27;</span>)</span><br><span class="line"><span class="comment"># \d\w之类的一般要在&#x27;&#x27;前加 r</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><h5 id="re-finditer：-重点，用的最多"><a href="#re-finditer：-重点，用的最多" class="headerlink" title="re.finditer：(重点，用的最多)"></a>re.finditer：(重点，用的最多)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">result = re.finditer(<span class="string">r&#x27;\d+&#x27;</span>, <span class="string">&#x27;我今年18岁，我存款有2000元&#x27;</span>)</span><br><span class="line"><span class="comment"># .finditer(返回类型是迭代器)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> result:<span class="comment"># 从迭代器中拿到内容</span></span><br><span class="line">    <span class="built_in">print</span>(item.group())<span class="comment"># 从匹配的结果中拿到数据</span></span><br><span class="line">    <span class="comment"># .group() 拿到的数据是从re.Match object中转换的</span></span><br></pre></td></tr></table></figure><h5 id="re-search："><a href="#re-search：" class="headerlink" title="re.search："></a>re.search：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">result = re.search(<span class="string">r&#x27;\d&#x27;</span>, <span class="string">&#x27;我叫周杰伦，今年32岁，我的班级是5年4班&#x27;</span>)</span><br><span class="line"><span class="comment"># .search(匹配只会返回第一次匹配到的内容)</span></span><br><span class="line"><span class="built_in">print</span>(result.group())</span><br></pre></td></tr></table></figure><h5 id="re-match："><a href="#re-match：" class="headerlink" title="re.match："></a>re.match：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">result = re.match(<span class="string">r&#x27;\d&#x27;</span>, <span class="string">&#x27;我叫周杰伦，今年32岁，我的班级是5年4班&#x27;</span>)</span><br><span class="line"><span class="comment"># .match(在匹配的时候，是从字符串的开头进行匹配的，类似在正则前面加上了^)</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><h5 id="预加载："><a href="#预加载：" class="headerlink" title="预加载："></a>预加载：</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 预加载，提前把正则对象加载完毕</span></span><br><span class="line">obj = re.<span class="built_in">compile</span>(<span class="string">r&#x27;\d+&#x27;</span>)</span><br><span class="line">result = obj.findall(<span class="string">&#x27;xxxxxxxxxxxxxxxxx&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">s = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;div class=&#x27;西游记&#x27;&gt;&lt;span id=&#x27;10010&#x27;&gt;中国联通&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&lt;div class=&#x27;西游记&#x27;&gt;&lt;span id=&#x27;10086&#x27;&gt;中国移动&lt;/span&gt;&lt;/div&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">obj = re.<span class="built_in">compile</span>(<span class="string">r&#x27;&lt;span id= &#x27;</span>(?P&lt;<span class="built_in">id</span>&gt;\d+)<span class="string">&#x27;&gt; (?P&lt;name&gt;.*?)&lt;/span&gt;&#x27;</span> )</span><br><span class="line"><span class="comment"># ?P&lt;xxx&gt; 把数据存入到XXX里面</span></span><br><span class="line">result = obj.finditer(s)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> result:</span><br><span class="line">    <span class="built_in">id</span> = item.group(<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">    name = item.group(<span class="string">&#x27;name&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">id</span>)</span><br><span class="line">    <span class="built_in">print</span>(name)</span><br></pre></td></tr></table></figure><h3 id="HTML基础语法"><a href="#HTML基础语法" class="headerlink" title="HTML基础语法"></a>HTML基础语法</h3><p>1、&lt;标签名 属性=”值”&gt;被标记的内容&lt;/标签名&gt;</p><p><img src="https://gitee.com/polaris_tyh/pic/raw/master/img/image-20211125113015109.png" alt="image-20211125113015109"></p><p>2、&lt;标签名 属性=”值” /&gt;</p><p><img src="https://gitee.com/polaris_tyh/pic/raw/master/img/image-20211125113208415.png" alt="image-20211125113208415"></p><h3 id="CSS基础语法"><a href="#CSS基础语法" class="headerlink" title="CSS基础语法"></a>CSS基础语法</h3><h4 id="1、css语法规则："><a href="#1、css语法规则：" class="headerlink" title="1、css语法规则："></a>1、css语法规则：</h4><p>​    1、通过style属性来编写样式</p><p>​    2、通过style标签，然后使用选择器的形式来编写样式</p><p>​    3、在css文件中编写样式，通过link引入该文件</p><h4 id="2、css选择器-重点"><a href="#2、css选择器-重点" class="headerlink" title="2、css选择器(重点)"></a>2、css选择器(重点)</h4><p>​    1、id选择器            #</p><p>​    2、标签选择器        标签</p><p>​    3、类选择器            .</p><p>​    4、选择器分组        ,</p><p>​    5、后代选择器        空格</p><p>​    6、子选择器            &gt;</p><p>​    7、相邻选择器        +</p><p>​    8、属性选择器        [属性=值]</p><h3 id="bs4学习"><a href="#bs4学习" class="headerlink" title="bs4学习"></a>bs4学习</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">html = <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">&lt;ul&gt;</span></span><br><span class="line"><span class="string">    &lt;li&gt;&lt;a href=&#x27;zhangwuji.com&#x27;&gt;张无忌&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">    &lt;li id=&#x27;abc&#x27;&gt;&lt;a href=&#x27;zhouxingchi.com&#x27;&gt;周星驰&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">    &lt;li&gt;&lt;a href=&#x27;zhubajie.com&#x27;&gt;猪八戒&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">    &lt;li&gt;&lt;a href=&#x27;wuzetian.com&#x27;&gt;武则天&lt;/a&gt;&lt;/li&gt;</span></span><br><span class="line"><span class="string">&lt;ul&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、初始化beautifulSoup对象</span></span><br><span class="line">page = BeautifulSoup(html, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># page.find(&#x27;标签名&#x27;, attrs=&#123;&#x27;属性&#x27;: &#x27;值&#x27;&#125;)   查找某个元素,只会找到一个结果</span></span><br><span class="line"><span class="comment"># page.find_all(&#x27;标签名&#x27;, attrs=&#123;&#x27;属性&#x27;: &#x27;值&#x27;&#125;)   找到一堆结果</span></span><br><span class="line"></span><br><span class="line">li_list = page.find_all(<span class="string">&#x27;li&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># print(a.text)   # 拿a标签里面的文本</span></span><br><span class="line"><span class="comment"># print(a.get(&#x27;href&#x27;))    # 拿属性.get(&#x27;属性名&#x27;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用一个循环把文本和属性遍历出来</span></span><br><span class="line"><span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">    a = li.find(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    a_text = a.text</span><br><span class="line">    href = a.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(a_text, href)</span><br></pre></td></tr></table></figure><h4 id="bs4实战"><a href="#bs4实战" class="headerlink" title="bs4实战"></a>bs4实战</h4><h5 id="旧版新地网页获取数据"><a href="#旧版新地网页获取数据" class="headerlink" title="旧版新地网页获取数据"></a>旧版新地网页获取数据</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"><span class="comment"># 提取url</span></span><br><span class="line">url = <span class="string">&#x27;https://www.construdip.com/marketanalysis/0/list/1.shtml&#x27;</span></span><br><span class="line"></span><br><span class="line">responses = requests.get(url)</span><br><span class="line"><span class="comment"># 初始化BS4对象</span></span><br><span class="line">page = BeautifulSoup(responses.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打开一个文档</span></span><br><span class="line">f = <span class="built_in">open</span>(<span class="string">&#x27;./旧版新菜地.text&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从url里面的网页源代码提取 &#x27;table&#x27; 标签筛选出属性是 &#x27;class=hq_table&#x27;的标签内容</span></span><br><span class="line">page_table = page.find(<span class="string">&#x27;table&#x27;</span>, attrs=&#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;hq_table&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到储存到page_table里面的 &#x27;tr&#x27;标签</span></span><br><span class="line">page_tr = page_table.find_all(<span class="string">&#x27;tr&#x27;</span>)</span><br><span class="line"><span class="comment"># print(page_tr)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行遍历查找</span></span><br><span class="line"><span class="keyword">for</span> trs <span class="keyword">in</span> page_tr:</span><br><span class="line">    page_td = trs.find_all(<span class="string">&#x27;td&#x27;</span>)</span><br><span class="line">    proName = page_td[<span class="number">0</span>].text</span><br><span class="line">    <span class="built_in">min</span> = page_td[<span class="number">1</span>].text</span><br><span class="line">    ave = page_td[<span class="number">2</span>].text</span><br><span class="line">    high = page_td[<span class="number">3</span>].text</span><br><span class="line">    spec = page_td[<span class="number">4</span>].text</span><br><span class="line">    unit = page_td[<span class="number">5</span>].text</span><br><span class="line">    data = page_td[<span class="number">6</span>].text</span><br><span class="line">    <span class="comment"># print(proName, min, ave, high, spec, unit, data)</span></span><br><span class="line">    <span class="comment"># 把提取到的内容写入到f文档里面</span></span><br><span class="line">    f.write(<span class="string">f&#x27;<span class="subst">&#123;proName&#125;</span>, <span class="subst">&#123;<span class="built_in">min</span>&#125;</span>, <span class="subst">&#123;ave&#125;</span>, <span class="subst">&#123;high&#125;</span>, <span class="subst">&#123;spec&#125;</span>, <span class="subst">&#123;unit&#125;</span>, <span class="subst">&#123;data&#125;</span>\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭进程</span></span><br><span class="line">f.close()</span><br><span class="line">responses.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;结束!&#x27;</span>)</span><br></pre></td></tr></table></figure><h5 id="彼岸桌面壁纸图片爬取"><a href="#彼岸桌面壁纸图片爬取" class="headerlink" title="彼岸桌面壁纸图片爬取"></a>彼岸桌面壁纸图片爬取</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">page_nb = <span class="built_in">input</span>(<span class="string">&#x27;请输入你要爬取的页码:&#x27;</span>)</span><br><span class="line"></span><br><span class="line">url = <span class="string">f&#x27;http://www.netbian.com/index_<span class="subst">&#123;page_nb&#125;</span>.htm&#x27;</span></span><br><span class="line">url2 = <span class="string">&#x27;http://www.netbian.com&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">注意:</span></span><br><span class="line"><span class="string">    获取的子页面的url如果开头是/,直接在前面拼接上域名即可</span></span><br><span class="line"><span class="string">    获取的子页面的url如果不是开头,此时需要找到主页面的url,去掉url最后一个/后面的内容,然后拼接上获取的子页面url</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据处理</span></span><br><span class="line">responses = requests.get(url=url,headers=headers)</span><br><span class="line">responses.encoding = <span class="string">&#x27;gbk&#x27;</span></span><br><span class="line">lis = BeautifulSoup(responses.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取网页源代码里的&lt;div&gt;标签</span></span><br><span class="line">div = lis.find_all(<span class="string">&#x27;div&#x27;</span>, attrs=&#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;list&#x27;</span>&#125;)</span><br><span class="line"><span class="comment"># print(div)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行遍历筛选出&lt;a&gt;标签</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> div:</span><br><span class="line">    href = item.find_all(<span class="string">&#x27;a&#x27;</span>)</span><br><span class="line">    <span class="comment"># 进行遍历筛选出&lt;a&gt;标签里面的href链接</span></span><br><span class="line">    <span class="keyword">for</span> item2 <span class="keyword">in</span> href:</span><br><span class="line">        sb = item2.get(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        <span class="comment"># 对所有获取的href链接进一步筛选</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(sb) == <span class="number">15</span>:</span><br><span class="line">            <span class="comment"># 拼接获取到的url</span></span><br><span class="line">            url3 = url2+sb</span><br><span class="line">            <span class="comment"># 再一次进行对子页面的数据处理</span></span><br><span class="line">            url3_responses = requests.get(url=url3, headers=headers)</span><br><span class="line">            url3_responses.encoding = <span class="string">&#x27;gbk&#x27;</span></span><br><span class="line">            divs = BeautifulSoup(url3_responses.text, <span class="string">&#x27;html.parser&#x27;</span>)</span><br><span class="line">            <span class="comment"># 获取&lt;div class=&quot;list&quot;&gt;标签</span></span><br><span class="line">            div2 = divs.find(<span class="string">&#x27;div&#x27;</span>, attrs=&#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;pic&#x27;</span>&#125;)</span><br><span class="line">            <span class="comment"># 获取图片链接和图片名字</span></span><br><span class="line">            src = div2.find(<span class="string">&#x27;img&#x27;</span>).get(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">            name = div2.find(<span class="string">&#x27;img&#x27;</span>).get(<span class="string">&#x27;alt&#x27;</span>)</span><br><span class="line">            <span class="built_in">print</span>(name, <span class="string">&#x27; 爬取成功&#x27;</span>)</span><br><span class="line">            <span class="comment"># 对src的url进行数据爬取并进行持久化存储,图片不能用.text表现,只能用.content转换成字节进行保存</span></span><br><span class="line">            img_resp = requests.get(url=src, headers=headers)</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">f&#x27;F:/爬虫/图片爬取/<span class="subst">&#123;name&#125;</span>.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:    <span class="comment"># wb是写入字节的</span></span><br><span class="line">                f.write(img_resp.content)   <span class="comment"># .content 是把拿到的内容转换成字节的形式,图片可以显示</span></span><br><span class="line">    <span class="comment"># 关闭文件</span></span><br><span class="line">    f.close()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;爬取结束!&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>.text(返回的是字符串类型的响应数据)        写入为’w’</p><p>.content(返回的是二进制的响应数据)    可用于图片类型        写入为’wb’</p><p>.json()    (返回的是对象类型的响应数据)    可去网页进行json转换        写入为’w’</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">bs4</span><br><span class="line"><span class="bullet">-</span> bs4解析原理</span><br><span class="line"><span class="bullet">    -</span> 实例化一个BeautifulSoup的对象，且将待解析的页面源码数据加载到该对象中</span><br><span class="line"><span class="bullet">    -</span> 调用BeautifulSoup对象中相关方法或者属性进行标签定位和文本数据的提取</span><br><span class="line"><span class="bullet">-</span> 环境安装：</span><br><span class="line"><span class="bullet">    -</span> pip install lxml #解析器</span><br><span class="line"><span class="bullet">    -</span> pip install bs4</span><br><span class="line"><span class="bullet">-</span> BeautifulSoup对象的实例化：</span><br><span class="line"><span class="bullet">    -</span> fp = open(&#x27;./xxxx.html&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;)</span><br><span class="line"><span class="bullet">    -</span> BeautifulSoup(fp,&#x27;lxml&#x27;)：用来将本地存储的html文档中的数据进行解析</span><br><span class="line"><span class="bullet">    -</span> BeautifulSoup(page<span class="emphasis">_text，’lxml‘)：用来将互联网上请求到的页面源码数据进行解析</span></span><br><span class="line"><span class="emphasis">- 标签定位：</span></span><br><span class="line"><span class="emphasis">    - soup.tagName：只可以定位到第一次出现的tagName标签</span></span><br><span class="line"><span class="emphasis">    - soup.find(&#x27;tagName&#x27;,attrName=&#x27;value&#x27;):属性定位</span></span><br><span class="line"><span class="emphasis">    - soup.find_</span>all:跟find一样用作属性定位，只不过findAll返回的是列表</span><br><span class="line"><span class="bullet">    -</span> soup.select(&#x27;选择器&#x27;):选择器定位返回的是一个列表</span><br><span class="line"><span class="bullet">        -</span> class类选择器</span><br><span class="line"><span class="bullet">        -</span> id选择器</span><br><span class="line"><span class="bullet">        -</span> 标签选择器</span><br><span class="line"><span class="bullet">        -</span> 层级选择soup.select(&#x27;.xxx &gt; xx &gt; xx&#x27;)[0]</span><br><span class="line"><span class="bullet">            -</span> 大于号:表示一个层级</span><br><span class="line"><span class="bullet">            -</span> 空格：表示多个层级</span><br><span class="line"><span class="bullet">-</span> 取数据</span><br><span class="line"><span class="bullet">    -</span> .text/get<span class="emphasis">_text：返回的是该标签下所有的文本内容</span></span><br><span class="line"><span class="emphasis">    - .string:返回的是该标签直系的文本内容</span></span><br><span class="line"><span class="emphasis">- 取属性：</span></span><br><span class="line"><span class="emphasis">    - .soup.a[&#x27;href&#x27;]<span class="xml"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;xxxxx&quot;</span>&gt;</span></span>获取A标签的href属性值</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-div2 = divs.find(<span class="string">&#x27;div&#x27;</span>, class_=<span class="string">&#x27;pic&#x27;</span>)</span><br><span class="line">-div2 = divs.find(<span class="string">&#x27;div&#x27;</span>, attrs=&#123;<span class="string">&#x27;class&#x27;</span>: <span class="string">&#x27;pic&#x27;</span>&#125;)</span><br><span class="line">上面两个代码返回的内容都是一样的</span><br></pre></td></tr></table></figure><h3 id="xpath学习"><a href="#xpath学习" class="headerlink" title="xpath学习"></a>xpath学习</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">xpath解析</span><br><span class="line"><span class="bullet">-</span> 环境安装：</span><br><span class="line"><span class="bullet">    -</span> pip install lxml</span><br><span class="line"><span class="bullet">-</span> 解析原理:html标签是以树状的形式进行展示</span><br><span class="line"><span class="bullet">    -</span> 1.实例化一个etree的对象，且将待解析的页面源码数据加载到该对象中</span><br><span class="line"><span class="bullet">    -</span> 2.调用etree对象的xpath方法结合着不同的xpath表达式实现标签的定位和数据提取</span><br><span class="line"><span class="bullet">-</span> 实例化etree对象</span><br><span class="line"><span class="bullet">    -</span> from lxml import etree</span><br><span class="line"><span class="code">    1.将本地html文档加载到该对象中</span></span><br><span class="line"><span class="code">    - etree.parse(&#x27;filename&#x27;)</span></span><br><span class="line"><span class="code">    2.网站获取的页面数据加载到该对象</span></span><br><span class="line"><span class="code">    - etree.HTML(page_text)</span></span><br><span class="line"><span class="code">    - xpath(&#x27;xpath表达式的字符串&#x27;)</span></span><br><span class="line"><span class="code">- 标签定位：</span></span><br><span class="line"><span class="code">    - 最左侧的/:如果xpath表达式最左侧是以/开头则表示该xpath表达式一定要从根标签开始定位指定标签(忽略)</span></span><br><span class="line"><span class="code">    - 非最左侧的/:表示一个层级</span></span><br><span class="line"><span class="code">    - 非左侧的//:表示多个层级</span></span><br><span class="line"><span class="code">    - 最左侧的//：xpath表达式可以从任意位置进行标签定位</span></span><br><span class="line"><span class="code">    - 取某一个标签: /xxxxx[第几个标签]    /xxxxx[3] 取xxxx标签里的第三个标签</span></span><br><span class="line"><span class="code">    </span></span><br><span class="line"><span class="code">    - 属性定位：tree.xpath(&#x27;//div[@属性名称=&#x27;属性内容&#x27;]&#x27;)</span></span><br><span class="line"><span class="code">      tree.xpath(&#x27;//div[@class=&quot;xxx&quot;]&#x27;)</span></span><br><span class="line"><span class="code">    - 索引定位：tree.xpath(&#x27;//div[@class=&quot;xxx&quot;][3]&#x27;)取&lt;div class=&#x27;xxx&#x27;&gt;下的第3个标签 索引是从[1]开始</span></span><br><span class="line"><span class="code">    - 模糊匹配：</span></span><br><span class="line"><span class="code">        - //div[contains(@class, &quot;ng&quot;)]</span></span><br><span class="line"><span class="code">        - //div[starts-with(@class, &quot;ta&quot;)]</span></span><br><span class="line"><span class="code">- 取文本</span></span><br><span class="line"><span class="code">    - /text():获取的是标签中直系文本内容</span></span><br><span class="line"><span class="code">       tree.xpath(&#x27;//div[@class=&quot;xxx&quot;]/text()&#x27;)</span></span><br><span class="line"><span class="code">    - //text()：获取的是标签下所有的文本内容</span></span><br><span class="line"><span class="code">        tree.xpath(&#x27;//div[@class=&quot;xxx&quot;][3]//text()&#x27;)</span></span><br><span class="line"><span class="code">- 取属性</span></span><br><span class="line"><span class="code">    - /@attrName</span></span><br><span class="line"><span class="code">        tree.xpath(&#x27;//div[@class=&quot;xxx&quot;][3]/@href[数字]&#x27;)</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">通用处理中文乱码的解决方案</span><br><span class="line"><span class="code">xxxxx.encode(&#x27;iso-8859-1&#x27;).decode(;gbk)</span></span><br></pre></td></tr></table></figure><h4 id="创建文件夹-os模块"><a href="#创建文件夹-os模块" class="headerlink" title="创建文件夹 os模块"></a>创建文件夹 os模块</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个文件夹</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os .path exists(<span class="string">&#x27;路径地址/文件夹名字&#x27;</span>)  判断这个路径下是否有这个文件夹</span><br><span class="line">os.mkdir(<span class="string">&#x27;路径地址/文件夹名字&#x27;</span>)         如果没有这个该路径下没有这个文件夹就新建一个</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;xxx&#x27;</span>, <span class="string">&#x27;w/wb&#x27;</span>,encoding=<span class="string">&#x27;xxx&#x27;</span>)xxx是文件保存的地址, <span class="string">&#x27;w:文本/wb:二进制用于图片&#x27;</span>, encoding=编码格式</span><br><span class="line"><span class="comment"># 压缩包也是二进制形式写入</span></span><br><span class="line">fp.write(写入数据)</span><br></pre></td></tr></table></figure><h3 id="验证码识别"><a href="#验证码识别" class="headerlink" title="验证码识别"></a>验证码识别</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">验证码识别</span><br><span class="line"></span><br><span class="line">验证码和爬虫之间的爱恨情仇？</span><br><span class="line">反爬机制：验证码.识别验证码图片中的数据，用于模拟登陆操作。</span><br><span class="line"></span><br><span class="line">识别验证码的操作：</span><br><span class="line"><span class="bullet">    -</span> 人工肉眼识别。（不推荐）</span><br><span class="line"><span class="bullet">    -</span> 第三方自动识别（推荐）</span><br><span class="line"><span class="bullet">        -</span> 云打码：http://www.yundama.com/demo.html</span><br><span class="line">云打码的使用流程：</span><br><span class="line"><span class="bullet">    -</span> 注册：普通和开发者用户</span><br><span class="line"><span class="bullet">    -</span> 登录：</span><br><span class="line"><span class="bullet">        -</span> 普通用户的登录：查询该用户是否还有剩余的题分</span><br><span class="line"><span class="bullet">        -</span> 开发者用户的登录：</span><br><span class="line"><span class="bullet">            -</span> 创建一个软件：我的软件-》添加新软件-》录入软件名称-》提交（软件id和秘钥）</span><br><span class="line"><span class="bullet">            -</span> 下载示例代码：开发文档-》点此下载：云打码接口DLL-》PythonHTTP示例下载</span><br><span class="line">实战：识别古诗文网登录页面中的验证码。</span><br><span class="line">使用打码平台识别验证码的编码流程：</span><br><span class="line"><span class="bullet">    -</span> 将验证码图片进行本地下载</span><br><span class="line"><span class="bullet">    -</span> 调用平台提供的示例代码进行图片数据识别</span><br></pre></td></tr></table></figure><p>判断是否响应成功可以使用:  print(xxxxx.status_code)</p><h3 id="requests模块高级"><a href="#requests模块高级" class="headerlink" title="requests模块高级"></a>requests模块高级</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">模拟登录：</span><br><span class="line"><span class="bullet">    -</span> 爬取基于某些用户的用户信息。</span><br><span class="line">需求：对人人网进行模拟登录。</span><br><span class="line"><span class="bullet">    -</span> 点击登录按钮之后会发起一个post请求</span><br><span class="line"><span class="bullet">    -</span> post请求中会携带登录之前录入的相关的登录信息（用户名，密码，验证码......）</span><br><span class="line"><span class="bullet">    -</span> 验证码：每次请求都会变化</span><br><span class="line"></span><br><span class="line">需求：爬取当前用户的相关的用户信息（个人主页中显示的用户信息）</span><br><span class="line"></span><br><span class="line">http/https协议特性：无状态。</span><br><span class="line">没有请求到对应页面数据的原因：</span><br><span class="line"><span class="code">    发起的第二次基于个人主页页面请求的时候，服务器端并不知道该此请求是基于登录状态下的请求。</span></span><br><span class="line"><span class="code">cookie：用来让服务器端记录客户端的相关状态。</span></span><br><span class="line"><span class="code">    - 手动处理：通过抓包工具获取cookie值，将该值封装到headers中。（不建议）</span></span><br><span class="line"><span class="code">    - 自动处理：</span></span><br><span class="line"><span class="code">        - cookie值的来源是哪里？</span></span><br><span class="line"><span class="code">            - 模拟登录post请求后，由服务器端创建。</span></span><br><span class="line"><span class="code">        session会话对象：</span></span><br><span class="line"><span class="code">            - 作用：</span></span><br><span class="line"><span class="code">                1.可以进行请求的发送。</span></span><br><span class="line"><span class="code">                2.如果请求过程中产生了cookie，则该cookie会被自动存储/携带在该session对象中。</span></span><br><span class="line"><span class="code">        - 创建一个session对象：session = requests.Session()</span></span><br><span class="line"><span class="code">        - 使用session对象进行模拟登录post请求的发送（cookie就会被存储在session中）</span></span><br><span class="line"><span class="code">        - session对象对个人主页对应的get请求进行发送（携带了cookie）</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">代理：破解封IP这种反爬机制。</span><br><span class="line">什么是代理：</span><br><span class="line"><span class="bullet">    -</span> 代理服务器。</span><br><span class="line">代理的作用：</span><br><span class="line"><span class="bullet">    -</span> 突破自身IP访问的限制。</span><br><span class="line"><span class="bullet">    -</span> 隐藏自身真实IP</span><br><span class="line">代理相关的网站：</span><br><span class="line"><span class="bullet">    -</span> 快代理</span><br><span class="line"><span class="bullet">    -</span> 西祠代理</span><br><span class="line"><span class="bullet">    -</span> www.goubanjia.com</span><br><span class="line">代理ip的类型：</span><br><span class="line"><span class="bullet">    -</span> http：应用到http协议对应的url中</span><br><span class="line"><span class="bullet">    -</span> https：应用到https协议对应的url中</span><br><span class="line"></span><br><span class="line">代理ip的匿名度：</span><br><span class="line"><span class="bullet">    -</span> 透明：服务器知道该次请求使用了代理，也知道请求对应的真实ip</span><br><span class="line"><span class="bullet">    -</span> 匿名：知道使用了代理，不知道真实ip</span><br><span class="line"><span class="bullet">    -</span> 高匿：不知道使用了代理，更不知道真实的ip</span><br></pre></td></tr></table></figure><h4 id="爬取人人网当前用户的个人详情页数据"><a href="#爬取人人网当前用户的个人详情页数据" class="headerlink" title="爬取人人网当前用户的个人详情页数据"></a>爬取人人网当前用户的个人详情页数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编码流程：</span></span><br><span class="line"><span class="comment"># 1.验证码的识别，获取验证码图片的文字数据</span></span><br><span class="line"><span class="comment"># 2.对post请求进行发送（处理请求参数）</span></span><br><span class="line"><span class="comment"># 3.对响应数据进行持久化存储</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> CodeClass <span class="keyword">import</span> YDMHttp</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 封装识别验证码图片的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getCodeText</span>(<span class="params">imgPath, codeType</span>):</span></span><br><span class="line">    <span class="comment"># 普通用户用户名</span></span><br><span class="line">    username = <span class="string">&#x27;bobo328410948&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 普通用户密码</span></span><br><span class="line">    password = <span class="string">&#x27;bobo328410948&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 软件ＩＤ，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appid = <span class="number">6003</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 软件密钥，开发者分成必要参数。登录开发者后台【我的软件】获得！</span></span><br><span class="line">    appkey = <span class="string">&#x27;1f4b564483ae5c907a1d34f8e2f2776c&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 图片文件：即将被识别的验证码图片的路径</span></span><br><span class="line">    filename = imgPath</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 验证码类型，# 例：1004表示4位字母数字，不同类型收费不同。请准确填写，否则影响识别率。在此查询所有类型 http://www.yundama.com/price.html</span></span><br><span class="line">    codetype = codeType</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 超时时间，秒</span></span><br><span class="line">    timeout = <span class="number">20</span></span><br><span class="line">    result = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 检查</span></span><br><span class="line">    <span class="keyword">if</span> (username == <span class="string">&#x27;username&#x27;</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;请设置好相关参数再测试&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 初始化</span></span><br><span class="line">        yundama = YDMHttp(username, password, appid, appkey)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 登陆云打码</span></span><br><span class="line">        uid = yundama.login();</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;uid: %s&#x27;</span> % uid)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 查询余额</span></span><br><span class="line">        balance = yundama.balance();</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;balance: %s&#x27;</span> % balance)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 开始识别，图片路径，验证码类型ID，超时时间（秒），识别结果</span></span><br><span class="line">        cid, result = yundama.decode(filename, codetype, timeout);</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;cid: %s, result: %s&#x27;</span> % (cid, result))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个session对象</span></span><br><span class="line">session = requests.Session()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.对验证码图片进行捕获和识别</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">url = <span class="string">&#x27;http://www.renren.com/SysHome.do&#x27;</span></span><br><span class="line">page_text = requests.get(url=url, headers=headers).text</span><br><span class="line">tree = etree.HTML(page_text)</span><br><span class="line">code_img_src = tree.xpath(<span class="string">&#x27;//*[@id=&quot;verifyPic_login&quot;]/@src&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">code_img_data = requests.get(url=code_img_src, headers=headers).content</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;./code.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(code_img_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用云打码提供的示例代码对验证码图片进行识别</span></span><br><span class="line">result = getCodeText(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求的发送（模拟登录）</span></span><br><span class="line">login_url = <span class="string">&#x27;http://www.renren.com/ajaxLogin/login?1=1&amp;uniqueTimestamp=2019431046983&#x27;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>: <span class="string">&#x27;www.zhangbowudi@qq.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;icode&#x27;</span>: result,</span><br><span class="line">    <span class="string">&#x27;origURL&#x27;</span>: <span class="string">&#x27;http://www.renren.com/home&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;domain&#x27;</span>: <span class="string">&#x27;renren.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;key_id&#x27;</span>: <span class="string">&#x27;1&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;captcha_type&#x27;</span>: <span class="string">&#x27;web_login&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;password&#x27;</span>: <span class="string">&#x27;06768edabba49f5f6b762240b311ae5bfa4bcce70627231dd1f08b9c7c6f4375&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;rkey&#x27;</span>: <span class="string">&#x27;3d1f9abdaae1f018a49d38069fe743c8&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;f&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 使用session进行post请求的发送</span></span><br><span class="line">response = session.post(url=login_url, headers=headers, data=data)</span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 爬取当前用户的个人主页对应的页面数据</span></span><br><span class="line">detail_url = <span class="string">&#x27;http://www.renren.com/289676607/profile&#x27;</span></span><br><span class="line"><span class="comment"># 手动cookie处理</span></span><br><span class="line"><span class="comment"># headers = &#123;</span></span><br><span class="line"><span class="comment">#     &#x27;Cookie&#x27;:&#x27;xxxx&#x27;</span></span><br><span class="line"><span class="comment"># &#125;</span></span><br><span class="line"><span class="comment"># 使用携带cookie的session进行get请求的发送</span></span><br><span class="line">detail_page_text = session.get(url=detail_url, headers=headers).text</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;bobo.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(detail_page_text)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="同步爬虫"><a href="#同步爬虫" class="headerlink" title="同步爬虫"></a>同步爬虫</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">urls = [</span><br><span class="line">    <span class="string">&#x27;http://www.netbian.com/1920x1080/&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://www.netbian.com/1920x1080/index_2.htm&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;http://www.netbian.com/1920x1080/index_3.htm&#x27;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.45 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># resp = requests.get(url=urls[0], headers=headers)</span></span><br><span class="line"><span class="comment"># print(resp)</span></span><br><span class="line"><span class="comment"># 定义一个get_content函数进行对url获取的数据进行字节处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_content</span>(<span class="params">url</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;正在爬取:&#x27;</span>, url)</span><br><span class="line">    resp = requests.get(url=url, headers=headers)</span><br><span class="line">    <span class="comment"># print(resp.status_code)</span></span><br><span class="line">    <span class="keyword">if</span> resp.status_code == <span class="number">200</span>: <span class="comment"># 判断是否url连通</span></span><br><span class="line">        <span class="keyword">return</span> resp.content     <span class="comment"># 返回的数据为字节形式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个parse_content函数进行对get_content获得的数据字节进行长度识别</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_content</span>(<span class="params">content</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;响应数据长度为:&#x27;</span>, <span class="built_in">len</span>(content))     <span class="comment"># 查询url字节的长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对urls列表进行遍历</span></span><br><span class="line"><span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">    content = get_content(url)</span><br><span class="line">    parse_content(content)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">高性能异步爬虫</span><br><span class="line">目的：在爬虫中使用异步实现高性能的数据爬取操作。</span><br><span class="line"></span><br><span class="line">异步爬虫的方式：</span><br><span class="line"><span class="bullet">    -</span> 1.多线程，多进程（不建议）：</span><br><span class="line"><span class="code">        好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。</span></span><br><span class="line"><span class="code">        弊端：无法无限制的开启多线程或者多进程。</span></span><br><span class="line"><span class="code">    - 2.线程池、进程池（适当的使用）：</span></span><br><span class="line"><span class="code">        好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。</span></span><br><span class="line"><span class="code">        弊端：池中线程或进程的数量是有上限。</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="bullet">    -</span> 3.单线程+异步协程（推荐）：</span><br><span class="line"><span class="code">        event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，</span></span><br><span class="line"><span class="code">        当满足某些条件的时候，函数就会被循环执行。</span></span><br><span class="line"><span class="code">        </span></span><br><span class="line"><span class="code">        coroutine：协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。</span></span><br><span class="line"><span class="code">        我们可以使用 async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回</span></span><br><span class="line"><span class="code">        一个协程对象。</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        task：任务，它是对协程对象的进一步封装，包含了任务的各个状态。</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        future：代表将来执行或还没有执行的任务，实际上和 task 没有本质区别。</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        async 定义一个协程.</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        await 用来挂起阻塞方法的执行。</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">占位符%s既可以表示字符串str,还可以表示整数int,浮点数float;</span><br><span class="line"></span><br><span class="line">占位符%d既可以表示整数int,还可以表示浮点数float(去除整数部分);</span><br><span class="line"></span><br><span class="line">占位符%f既可以表示浮点数float,还可以表示整数int(默认保留6位小数);</span><br></pre></td></tr></table></figure><p>保存文件如果路径中的 ‘ / ‘ 不换成 ‘ \ ‘ 那么可以在路径  ‘ D:\迅雷下载 ‘ 前面加个  ‘r’  即  r ‘D:\迅雷下载 ‘</p><h3 id="excal数据保存"><a href="#excal数据保存" class="headerlink" title="excal数据保存"></a>excal数据保存</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xlwt</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">workbook = xlwt.Workbook(encoding=&#x27;utf-8&#x27;)  # 创建workbook对象  相当于文件</span></span><br><span class="line"><span class="string">worksheet = workbook.add_sheet(&#x27;sheet1&#x27;)    # 创建工作表,工作表名字为&#x27;sheet1&#x27;</span></span><br><span class="line"><span class="string">worksheet.write(0, 0, &#x27;hello&#x27;)    # 写入数据,第一个代表&#x27;行&#x27;,第二个代表&#x27;列&#x27;,第三个是参数内容</span></span><br><span class="line"><span class="string">workbook.save(&#x27;students.xls&#x27;)   # 保存到本地目录下,文件名为  students.xls</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将99乘法表导入到excal表格中</span></span><br><span class="line">workbook = xlwt.Workbook(encoding=<span class="string">&#x27;utf-8&#x27;</span>)  <span class="comment"># 创建workbook对象  相当于文件</span></span><br><span class="line">worksheet = workbook.add_sheet(<span class="string">&#x27;sheet1&#x27;</span>)  <span class="comment"># 创建工作表,工作表名字为&#x27;sheet1&#x27;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">9</span>):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, i + <span class="number">1</span>):</span><br><span class="line">        worksheet.write(i, j, <span class="string">&#x27;%d * %d = %d&#x27;</span> % ((j+<span class="number">1</span>), (i+<span class="number">1</span>), (i+<span class="number">1</span>) * (j+<span class="number">1</span>)))</span><br><span class="line">workbook.save(<span class="string">&#x27;students.xls&#x27;</span>)  <span class="comment"># 保存到本地目录下,文件名为  students.xls</span></span><br></pre></td></tr></table></figure><h1 id="Scrapy学习"><a href="#Scrapy学习" class="headerlink" title="Scrapy学习"></a>Scrapy学习</h1><p>scrapy框架</p><ul><li><p>什么是框架?</p><ul><li>就是一个集成了很多功能并且具有很强通用性的一个项目模板.</li></ul></li><li><p>如何学习框架?</p><ul><li>专门学习框架封装的各种功能的详细用法.</li></ul></li><li><p>什么是scrapy?</p><ul><li>爬虫中封装好的一个明星框架.功能:高性能的持久化存储,异步的数据下载,高性能的数据解析,分布式</li></ul></li><li><p>scrapy框架的基础使用</p><ul><li>环境的安装:<ul><li>mac or linux : pip install scrapy </li><li>windows :<ul><li>pip install wheel</li><li>下载twisted, 下载地址:<a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted">http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</a></li><li>安装twisted: pip install xxxxxxxx.whl</li><li>pip install pywin32</li><li>pip install scrapy</li></ul></li></ul></li></ul></li></ul><p>​            测试:在终端里录入scrapy指令,没有报错即表示安装成功!</p><ul><li><p>创建一个工程:scrapy startproject xxxxxx(项目名字)</p></li><li><p>创建一个爬虫-&gt;必须要在spiders子目录中:scrapy genspider spiderName(爬虫名字) <a href="http://www.xxxx.com/">www.xxxx.com</a></p></li><li><p>执行工程:scrapy crawl spiderName       -&gt;scrapy crawl spiderName –nolog(不打印日志)</p><ul><li><p>在settings配置文件中加入一条: LOG_LEVEL = ‘ERROR’ 只显示错误信息</p><ul><li><p>修改settings配置文件中的  ROBOTSTXT_OBEY = True    -&gt;  ROBOTSTXT_OBEY = False</p></li><li><p>UA伪装,修改settings配置文件中的 USER_AGENT</p></li></ul></li></ul></li><li><p>scrapy持久化存储</p><ul><li>基于终端指令:<ul><li>要求: 只可以将parse方法的返回值存储到本地的文本文件中</li><li>注意: 持久化存储对应的文本文件的类型只可以为: ‘json’, ‘jsonlines’, ‘jl’, ‘csv’, ‘xml’, ‘marshal’, ‘pickle’</li><li>指令: scrapy crawl fileName -o filePath</li><li>好处: 简介高效便捷</li><li>缺点: 局限性比较强(数据只可以存储到指定后缀的文本文件中)</li></ul></li><li>基于管道: <ul><li>编码解析:<ul><li>数据解析</li><li>在item类中定义相关的属性</li><li>将解析的数据封装存储到item类型的对象</li><li>将item类型的对象提交给管道进行持久化存储的操作</li><li>在管道类的process_item中要将其接受到的item对象中存储的数据进行持久化存储操作</li><li>在配置文件中开启管道</li></ul></li></ul></li></ul></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/01/20/hello-world/"/>
      <url>/2022/01/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
